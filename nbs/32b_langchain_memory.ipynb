{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOI0nzfoTtoNxvOBT4EYPV8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gnoparus/bualabs/blob/master/nbs/32b_langchain_memory.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bkOsYAPmPxNf"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "_7oHXYXCP7ym"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we will begin exploring Conversational Memory for LLMs using Langchain.\n"
      ],
      "metadata": {
        "id": "WrAXCBTSH1VF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Import Library\n",
        "\n",
        "Install OpenAI and LangChain Library by Uncomment Cell below and Run (Shift + Enter)"
      ],
      "metadata": {
        "id": "2tV5b_GLGsfT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "AQWl4NIQ8LJm"
      },
      "outputs": [],
      "source": [
        "# ! pip install langchain\n",
        "# !pip install openai\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import the LangChain Library"
      ],
      "metadata": {
        "id": "5CS0QvZ48mTQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.chains import ConversationChain\n",
        "from langchain.memory import ConversationBufferMemory"
      ],
      "metadata": {
        "id": "lkYOdfr4Zphk"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Authentication\n",
        "\n",
        "Before we can access and utilize ChatGPT and GPT-4 from OpenAI, we need to register, sign up, and provide the necessary payment information.\n",
        "\n",
        "## Sign up\n",
        "\n",
        "Click on the green Sign up button in the link below to register for OpenAI.\n",
        "\n",
        "https://platform.openai.com/overview\n",
        "\n",
        "Please enter your credit card information on the Billing page.\n",
        "\n",
        "https://platform.openai.com/account/billing/payment-methods\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "MyBgjLga_6rG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## API Key\n",
        "\n",
        "To generate an API Key, please follow the steps below:\n",
        "\n",
        "1. Access the link provided below.\n",
        "2. Click on the \"Create new secret key\" button.\n",
        "3. Give a name to the key.\n",
        "4. Copy the secret key generated and keep it ready to be entered in the \"OPENAI_API_KEY\" field below.\n",
        "\n",
        "Link: https://platform.openai.com/account/api-keys\n",
        "\n",
        "Please make sure to securely store and protect your API Key as it grants access to the OpenAI API.\n",
        "\n",
        "\n",
        "## Sample API Key \n",
        "\n",
        "OPENAI_API_KEY: sk-abcDeFGHijKlMNopQrstuvWxyZ1234567890abcDefhiJklm\n",
        "\n",
        "Please execute the cell below and enter the secret key.\n"
      ],
      "metadata": {
        "id": "2jWLUnvhJ0cX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from getpass import getpass\n",
        "\n",
        "token = getpass('Enter OPENAI_API_KEY here: ')\n",
        "# print ('token is', token)\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = token"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "od_JGCDH85e8",
        "outputId": "62010e72-132f-4c31-dce7-d19d6f5fd536"
      },
      "execution_count": 4,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter OPENAI_API_KEY here: ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "After completing the testing, it is recommended to go back and delete the Secret Key on the page https://platform.openai.com/account/api-keys every time for security purposes. We can generate new keys continuously and without limitations."
      ],
      "metadata": {
        "id": "6DGekg9eQyD0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3. LangChain Memory"
      ],
      "metadata": {
        "id": "YpsFGTC4CIPp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.1 ConversationBufferMemory"
      ],
      "metadata": {
        "id": "78qNgpfz8_gu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The ConversationBufferMemory is a tool that can save messages and then retrieve them using a variable."
      ],
      "metadata": {
        "id": "812DM04TzWsN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will initialize ChatOpenAI with temperature=0.0 to make it repeatable. Please note that verbose=True."
      ],
      "metadata": {
        "id": "SGU8J6yp-zTu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chatllm = ChatOpenAI(temperature=0.0)\n",
        "memory = ConversationBufferMemory()\n",
        "conversation = ConversationChain(\n",
        "    llm=chatllm, \n",
        "    memory = memory,\n",
        "    verbose=True\n",
        ")"
      ],
      "metadata": {
        "id": "lbdx3md283jb"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, we'll start by greeting each other to test how everything functions. You'll notice a prompt LangChain set up behind the scenes in green."
      ],
      "metadata": {
        "id": "uvyTE3r7ri28"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "conversation.predict(input=\"Hello, my name is Ken\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 263
        },
        "id": "cJ149bud9Pj_",
        "outputId": "379f83e0-03ac-425f-ca7c-fe8a44ff30bf"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "\n",
            "Human: Hello, my name is Ken\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Hello Ken, it's nice to meet you. My name is AI. How can I assist you today?\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then a little arithmetic. \n",
        "\n",
        "You can see LangChain send all previous prompts to LLM."
      ],
      "metadata": {
        "id": "eKoCmowPs33A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "conversation.predict(input=\"What is 1+1?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "id": "riE1s3Zl9PoV",
        "outputId": "76452c48-75d6-4587-b54a-c81304744404"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "Human: Hello, my name is Ken\n",
            "AI: Hello Ken, it's nice to meet you. My name is AI. How can I assist you today?\n",
            "Human: What is 1+1?\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The answer to 1+1 is 2.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Even if the language model is stateless, it can still use the information from the previous chat message to answer the question."
      ],
      "metadata": {
        "id": "loTvfZmmtmlx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "conversation.predict(input=\"What is my name?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 315
        },
        "id": "UFQake5t9PxD",
        "outputId": "c7cbfe52-42d0-4801-d002-c0295cca4084"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "Human: Hi, my name is Ken\n",
            "AI: Hello Ken, it's nice to meet you. My name is AI. How can I assist you today?\n",
            "Human: What is 1+1?\n",
            "AI: The answer to 1+1 is 2.\n",
            "Human: What is my name?\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Your name is Ken, as you mentioned earlier.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can take a look inside the memory (chat log)."
      ],
      "metadata": {
        "id": "FxeuKMfeuDHs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(memory.buffer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x13bZnHD9Px2",
        "outputId": "f3f9d327-cc42-4966-c0cc-399ed97dabcd"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Human: Hello, my name is Ken\n",
            "AI: Hello Ken, it's nice to meet you. My name is AI. How can I assist you today?\n",
            "Human: What is 1+1?\n",
            "AI: The answer to 1+1 is 2.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We could retrive all the history buffer."
      ],
      "metadata": {
        "id": "M51UKLqpuX17"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "memory.load_memory_variables({})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jM3jufrf9Pzc",
        "outputId": "aa1ef880-179c-41ad-9e0a-88835fc9b0cd"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'history': \"Human: Hello, my name is Ken\\nAI: Hello Ken, it's nice to meet you. My name is AI. How can I assist you today?\\nHuman: What is 1+1?\\nAI: The answer to 1+1 is 2.\"}"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Craft new memory"
      ],
      "metadata": {
        "id": "tut5aKR4ymuk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have the option to create new memories in any way we prefer."
      ],
      "metadata": {
        "id": "nc1_i68uvFTN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "memory = ConversationBufferMemory()"
      ],
      "metadata": {
        "id": "__HkGZOC9P8N"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "memory.save_context({\"input\": \"Roses are blue.\"}, \n",
        "                    {\"output\": \"Violets are red.\"})"
      ],
      "metadata": {
        "id": "NpnMpGlO9QEl"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(memory.buffer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_vqb1Htf9QGf",
        "outputId": "43347370-306d-4172-dc06-48f412900d6b"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Human: Roses are blue.\n",
            "AI: Violets are red.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "memory.load_memory_variables({})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XKNcdzaaAMUp",
        "outputId": "c55deb0b-6627-422e-b4ad-16988ea54e5d"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'history': 'Human: Roses are blue.\\nAI: Violets are red.'}"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Add more memory."
      ],
      "metadata": {
        "id": "HuSc_l9nyN6h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "memory.save_context({\"input\": \"A poetic twist\"}, \n",
        "                    {\"output\": \"from what's often said.\"})"
      ],
      "metadata": {
        "id": "gu0xRXwfAO08"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "memory.load_memory_variables({})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eF1IgixzAQlc",
        "outputId": "6e6c6721-51db-4ec6-a780-f05aef9a3ebd"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'history': \"Human: Roses are blue.\\nAI: Violets are red.\\nHuman: A poetic twist\\nAI: from what's often said.\"}"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "34tO-SS_ASJS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.2 ConversationBufferWindowMemory"
      ],
      "metadata": {
        "id": "Itu4ZyWbAonF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ConversationBufferWindowMemory maintains a record of the conversation's interactions. The WINDOW only stores the most recent K interactions. This is helpful for keeping track of the latest interactions without letting the buffer become too large."
      ],
      "metadata": {
        "id": "HJdxf7KOyzaQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will import ConversationBufferWindowMemory from memory package"
      ],
      "metadata": {
        "id": "HmS1r4gDzg6u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.memory import ConversationBufferWindowMemory"
      ],
      "metadata": {
        "id": "KImB3DlKAl9Y"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set the window size (k) to 1."
      ],
      "metadata": {
        "id": "om96yt1q5wfo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "memory = ConversationBufferWindowMemory(k=1)      "
      ],
      "metadata": {
        "id": "hgIVyHYKAmBf"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "memory.save_context({\"input\": \"Roses are blue.\"}, \n",
        "                    {\"output\": \"Violets are red.\"})\n",
        "memory.save_context({\"input\": \"A poetic twist\"}, \n",
        "                    {\"output\": \"from what's often said.\"})"
      ],
      "metadata": {
        "id": "QCiSC07vAmMp"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The memory can only hold the most recent interaction."
      ],
      "metadata": {
        "id": "9cvS3_TT5uHh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "memory.load_memory_variables({})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Ho49m1QAmNq",
        "outputId": "625275d2-9561-44e9-93cf-2619f73858d9"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'history': \"Human: A poetic twist\\nAI: from what's often said.\"}"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will try the case above."
      ],
      "metadata": {
        "id": "MkavOtEa6TXt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatOpenAI(temperature=0.0)\n",
        "memory = ConversationBufferWindowMemory(k=1)\n",
        "conversation = ConversationChain(\n",
        "    llm=llm, \n",
        "    memory = memory,\n",
        "    verbose=False\n",
        ")"
      ],
      "metadata": {
        "id": "33_VELH2AmOy"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conversation.predict(input=\"Hi, my name is Ken\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Xw1CpSfcAmSL",
        "outputId": "50554311-dac1-4d26-fb75-bc91273e47d3"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Hello Ken, it's nice to meet you. My name is AI. How can I assist you today?\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conversation.predict(input=\"What is 1+1?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "WKU6WZsqAmV4",
        "outputId": "09115303-b734-4e51-f8a8-1a27e6d137e6"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The answer to 1+1 is 2.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# memory.load_memory_variables({})"
      ],
      "metadata": {
        "id": "7kaFWStL6_CK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The conversation memory doesn't contain a name, so the language model cannot respond to the question."
      ],
      "metadata": {
        "id": "qGhqsuv-6cLJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "conversation.predict(input=\"What is my name?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "IeP6aVWUAmam",
        "outputId": "3a7f4dd3-e8d7-4e2d-839c-5f295c092d2d"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"I'm sorry, I still don't have access to that information. However, I can tell you that your IP address is 192.168.1.1 and you are currently using a Chrome browser version 91.0.4472.124 on a Windows 10 operating system. Is there anything else I can help you with?\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "memory.load_memory_variables({})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pd1_0fxrAme8",
        "outputId": "37ed7dbe-1723-4c23-adad-7f4ebf06de92"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'history': \"Human: What is my name?\\nAI: I'm sorry, I still don't have access to that information. However, I can tell you that your IP address is 192.168.1.1 and you are currently using a Chrome browser version 91.0.4472.124 on a Windows 10 operating system. Is there anything else I can help you with?\"}"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.3 ConversationTokenBufferMemory"
      ],
      "metadata": {
        "id": "gJaXkOu8C2YR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The ConversationTokenBufferMemory stores recent interactions in memory. Instead of counting the number of interactions, it determines when to clear the interactions based on the length of tokens (words) used.\n",
        "\n",
        "The main distinction of ConversationTokenBufferMemory is that it sets a limit based on the number of words in the stored messages. This is unlike ConversationBufferWindowMemory, which removes interactions based on the number of turns."
      ],
      "metadata": {
        "id": "R1bdK2JoC7hW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ! pip install tiktoken"
      ],
      "metadata": {
        "id": "wWaZWqHqAmjK"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.memory import ConversationTokenBufferMemory\n",
        "from langchain.llms import OpenAI\n",
        "llm = ChatOpenAI(temperature=0.0)"
      ],
      "metadata": {
        "id": "aCJoQcE9Amuc"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set max_token_limit to 40"
      ],
      "metadata": {
        "id": "D3TxhMWs9h62"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "memory = ConversationTokenBufferMemory(llm=llm, max_token_limit=40)\n",
        "memory.save_context({\"input\": \"Doe\"},\n",
        "                    {\"output\": \"A deer, a female deer.\"})\n",
        "memory.save_context({\"input\": \"Ray\"},\n",
        "                    {\"output\": \"A drop of golden sun.\"})\n",
        "memory.save_context({\"input\": \"Me\"}, \n",
        "                    {\"output\": \"A name I call myself.\"})"
      ],
      "metadata": {
        "id": "AbhTY7I5AmvX"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "memory.load_memory_variables({})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xYG1DABeAmwl",
        "outputId": "fad2f64b-b38a-4788-f75d-4eaea8cef5dd"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'history': 'Human: Ray\\nAI: A drop of golden sun.\\nHuman: Me\\nAI: A name I call myself.'}"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set max_token_limit to 60"
      ],
      "metadata": {
        "id": "EvvVGJ079gj-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "memory = ConversationTokenBufferMemory(llm=llm, max_token_limit=60)\n",
        "memory.save_context({\"input\": \"Doe\"},\n",
        "                    {\"output\": \"A deer, a female deer.\"})\n",
        "memory.save_context({\"input\": \"Ray\"},\n",
        "                    {\"output\": \"A drop of golden sun.\"})\n",
        "memory.save_context({\"input\": \"Me\"}, \n",
        "                    {\"output\": \"A name I call myself.\"})"
      ],
      "metadata": {
        "id": "9crZHToh9Y9K"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "memory.load_memory_variables({})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a49157a2-0ee1-40a7-f4f9-d7e9ff16722f",
        "id": "5G-yqB039Y9L"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'history': 'Human: Doe\\nAI: A deer, a female deer.\\nHuman: Ray\\nAI: A drop of golden sun.\\nHuman: Me\\nAI: A name I call myself.'}"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.4 ConversationSummaryMemory"
      ],
      "metadata": {
        "id": "Kk_uMZsiF7CQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ConversationSummaryMemory is a kind of memory that summarizes the conversation as time goes on. It helps to condense information from the conversation over time.\n",
        "\n",
        "When the large language model runs, it includes the conversation history using these memory nodes. Instead of using the entire conversation history, we use ConversationSummaryMemory, which provides a summary of the conversation without losing important information. This helps to save space and stay within the token limit."
      ],
      "metadata": {
        "id": "E8eKiMTUF7hs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.memory import ConversationSummaryBufferMemory\n"
      ],
      "metadata": {
        "id": "kjbPQzNsGN9f"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create a long string\n",
        "news = \"\"\"\n",
        "A new six-story college campus building stands as Singapore’s latest ode to \\\n",
        "nature. Home to Nanyang Technological University’s (NTU) business school, the \\\n",
        "gently curved design features sunlit atriums, open-air study areas set against \\\n",
        "lush backdrops and elevators that descend into beds of tropical plants. \\\n",
        "Everything from handrails to benches, door frames to room dividers (and even \\\n",
        "                                                                    an adjoining bus stop), were built using wood.\n",
        "\n",
        "So, too, were the structural beams and columns. In fact, the building is \\\n",
        "made almost entirely from mass timber — a new generation of engineered wood, \\\n",
        "arranged in layers and bonded with strong adhesives, that is pushing \\\n",
        "the boundaries of architecture. Sprawling across 43,500 square meters \\\n",
        " (468,000 square feet), it is now Asia’s largest timber building, by floor area.\n",
        "\n",
        "Named Gaia, after the ancient Greek goddess of the Earth, the project \\\n",
        "opened in May and cost 125 million Singapore dollars ($93 million) to build. \\\n",
        "Its exposed timber frame is free from cladding or paint, a design \\\n",
        "decision that celebrates natural materials while giving visitors \\\n",
        "the feeling of walking between trees.\n",
        "\n",
        "According to the celebrated Japanese architect behind the project, \\\n",
        "Toyo Ito, this was precisely the point. “I always try to envision \\\n",
        "a connection with — and a feeling of — nature, such as trees and water, \\\n",
        "in my designs,” he told CNN shortly after the building’s inauguration \\\n",
        "ceremony. “The fact you mentioned that if feels like entering a forest \\\n",
        "shows that my vision came through.”\n",
        "\"\"\"\n",
        "\n",
        "memory = ConversationSummaryBufferMemory(llm=llm, max_token_limit=50)\n",
        "memory.save_context({\"input\": \"Hello\"}, {\"output\": \"What's up\"})\n",
        "memory.save_context({\"input\": \"Not much, just hanging\"},\n",
        "                    {\"output\": \"Cool\"})\n",
        "memory.save_context({\"input\": \"What is on the schedule today?\"}, \n",
        "                    {\"output\": f\"{news}\"})"
      ],
      "metadata": {
        "id": "k6ju7Ph4GOLT"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "memory.load_memory_variables({})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tmQACMrmGQDg",
        "outputId": "b68cbe09-1bdd-4173-88ec-367ad242f2ae"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'history': \"System: The human and AI engage in small talk before discussing the day's schedule. The AI informs the human of a morning meeting with the product team, time to work on the LangChain project, and a lunch meeting with a customer interested in the latest AI developments.\"}"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conversation = ConversationChain(\n",
        "    llm=llm, \n",
        "    memory = memory,\n",
        "    verbose=True\n",
        ")"
      ],
      "metadata": {
        "id": "0ElNP4LkGQb9"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conversation.predict(input=\"What would be a good demo to show?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        },
        "id": "yyWqcu5aGQxN",
        "outputId": "f8484b07-8d63-40ef-d04d-0a40e4064893"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "System: The human and AI engage in small talk before discussing the day's schedule. The AI informs the human of a morning meeting with the product team, time to work on the LangChain project, and a lunch meeting with a customer interested in the latest AI developments.\n",
            "Human: What would be a good demo to show?\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Based on the customer's interest in AI developments, I would suggest showcasing our latest natural language processing capabilities. We could demonstrate how our AI can understand and respond to complex language queries, and even provide personalized recommendations based on the user's preferences. Additionally, we could highlight our machine learning algorithms and how they continuously improve the accuracy and efficiency of our AI. Would you like me to prepare a demo for the meeting?\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jUZNJZx4GfPU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}